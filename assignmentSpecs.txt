Classification Assignment

2 Parts:

    Part 1 Due 2/22 + 10% | 3/1
        Create a decision tree inducer using C++ or Python
            -Your program should take in a csvfile and a list of which attributes
            you'd like to use in classification.
            -Your program should use either the gini index or gain-ratio as the
            selection criteria
            -Execution of your program should be as follows:
                $./myprog mydata.csv 1,2,3,6,7

                Where mydata.csv is in the same directory as the program
                1,2,3,6,7 are the attributes which we are selecting
                    (1 indexed; assuming there are 7 attributes)
                    (no need to handle case where user inputs attribute
                     which isn't listed)
            -output of program should be a text file outputting the decision tree
                
                I.e.
                    Root
                    |__Node1
                    |  |__Leaf1,1
                    |  |__Leaf1,2
                    |__Node2
                    |  |__Node2,1
                    |  |  |__Leaf2,1,1
                    |  |  |__Leaf2,1,2
                    |  |__Leaf2,2
                    |__Leaf3

                    (Using a depth-first based tree search is the easiest way to
                     output this, you can also use an online tool to do this
                     output method if you can find one. The point of the project
                     is the )
            -Your program should be capable of running on zeus.cs.txstate.edu
            -Your report should describe what you did, how you did it, any troubles
                    you had and examples of its output.
            -Page 333 has pseudocode for a basic decision tree.
            -Your report should be in a PDF, and include your code with your submission (or Github repo, if you submit a repo I will be looking at the master branch for 
            evaluation), and include the zeus-executable program in your submission, along with any test data you used to present in your report.
            -You may work with other students, but no copying (I will be looking over
            everyone's code)
            -Bonus if you can implement a postpruning method with it (and explain the process in your report with comparative examples of postpruning and no postpruning on
            the same dataset/attribute selection pair)
            -Bonus if your selection criterion can handle more than just categorical variables
                -Can either detect automatically, or be passed within the top of the CSV
                  file which type of variable it is


    Part 2 Due 2/15
        Use WEKA on UCI ML database sets
            -Collect 10 datasets from UCI ML database
            -Apply C4.5's decision tree inducer with 10-fold cross validation
                -report predicted accuracy and tree depth
            -Apply RandomTree's decision tree inducer wtih 10-fold cross val
                -report predicted accuracy and tree depth
            -Tabulate and compare the results of C4.5 and RandomTree
                -What are the differences you found? Explain, if possible.